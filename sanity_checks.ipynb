{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neurons That Panic — Sanity Checks Notebook\n",
        "\n",
        "This notebook contains validation checks for the adversarial trigger generation pipeline:\n",
        "\n",
        "- Check A: Objective monotonicity (R2, R3 improvements)\n",
        "- Check B: Tokenization sanity (pathological token detection)\n",
        "- Check C: Greedy vs beam/exhaustive search comparison\n",
        "- Check D: Marginal ablation analysis\n",
        "- Check E: Patch sanity pilot (1-token vs 3-token comparison)\n",
        "\n",
        "All checks validate that the adversarial generation process is working correctly and producing meaningful results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (for Colab)\n",
        "!pip install -q transformer_lens torch datasets matplotlib seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "from transformer_lens import HookedTransformer\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Seeds and constants\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Constants\n",
        "ARTIFACTS_DIR = \"artifacts\"\n",
        "MODEL_NAME = \"EleutherAI/pythia-160m-deduped\"\n",
        "FALLBACK_MODEL = \"EleutherAI/pythia-70m-deduped\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model (same configuration as main notebook)\n",
        "try:\n",
        "    model = HookedTransformer.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device=DEVICE,\n",
        "        dtype=DTYPE,\n",
        "        fold_ln=False,\n",
        "        center_writing_weights=False,\n",
        "        center_unembed=False,\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load {MODEL_NAME}: {e}\")\n",
        "    try:\n",
        "        model = HookedTransformer.from_pretrained(\n",
        "            FALLBACK_MODEL,\n",
        "            device=DEVICE,\n",
        "            dtype=DTYPE,\n",
        "            fold_ln=False,\n",
        "            center_writing_weights=False,\n",
        "            center_unembed=False,\n",
        "        )\n",
        "        MODEL_NAME = FALLBACK_MODEL\n",
        "    except Exception as e2:\n",
        "        print(f\"Failed to load fallback model: {e2}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata from artifacts/adv_metadata.jsonl\n",
        "metadata_file = os.path.join(ARTIFACTS_DIR, \"adv_metadata.jsonl\")\n",
        "\n",
        "if not os.path.exists(metadata_file):\n",
        "    raise FileNotFoundError(f\"Metadata file not found: {metadata_file}\")\n",
        "\n",
        "metadata_list = []\n",
        "with open(metadata_file, 'r') as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            metadata_list.append(json.loads(line))\n",
        "\n",
        "# validate schema and build DataFrame\n",
        "required_fields = ['id', 'clean_prompt', 'clean_label', 'adv_tokens', 'positions', \n",
        "                   'objective_k', 'relative_improvements', 'ablation_marginals', 'chosen_k']\n",
        "\n",
        "valid_entries = []\n",
        "for entry in metadata_list:\n",
        "    if all(field in entry for field in required_fields):\n",
        "        valid_entries.append(entry)\n",
        "    else:\n",
        "        missing = [f for f in required_fields if f not in entry]\n",
        "        print(f\"Warning: Entry {entry.get('id', 'unknown')} missing fields: {missing}\")\n",
        "\n",
        "metadata_list = valid_entries\n",
        "metadata_df = pd.DataFrame(metadata_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions (copied from main notebook for self-contained execution)\n",
        "\n",
        "def build_candidate_token_list(model, n_tokens=500):\n",
        "    \"\"\"Build a diverse list of candidate tokens that decode to actual words.\"\"\"\n",
        "    special_token_ids = {0}  # Token 0 is <|endoftext|>\n",
        "    if hasattr(model.tokenizer, 'all_special_ids'):\n",
        "        special_token_ids.update(model.tokenizer.all_special_ids)\n",
        "    \n",
        "    # sample from vocabulary ranges\n",
        "    ranges_to_check = [\n",
        "        (1000, 5000, 20),\n",
        "        (5000, 15000, 10),\n",
        "        (15000, 30000, 5),\n",
        "        (30000, 45000, 3),\n",
        "    ]\n",
        "    \n",
        "    good_tokens = []\n",
        "    for vocab_start, vocab_end, step in ranges_to_check:\n",
        "        for token_id in range(vocab_start, min(vocab_end, model.cfg.d_vocab), step):\n",
        "            if token_id in special_token_ids:\n",
        "                continue\n",
        "            \n",
        "            try:\n",
        "                token_str = model.to_string(torch.tensor([token_id]))\n",
        "                # filter valid tokens\n",
        "                if (token_str and\n",
        "                    len(token_str.strip()) > 0 and\n",
        "                    not token_str.startswith('<|') and\n",
        "                    not token_str.startswith('[') and\n",
        "                    len(token_str) < 15 and\n",
        "                    any(c.isalnum() for c in token_str)):\n",
        "                    good_tokens.append(token_id)\n",
        "                    if len(good_tokens) >= n_tokens:\n",
        "                        return good_tokens\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    return good_tokens[:n_tokens]\n",
        "\n",
        "\n",
        "def get_label_token_ids(model):\n",
        "    \"\"\"Get token IDs for 'negative' and 'positive' labels.\"\"\"\n",
        "    label_tokens = {}\n",
        "    for label_val, label_name in [(0, \"negative\"), (1, \"positive\")]:\n",
        "        try:\n",
        "            token_id = model.to_single_token(f\" {label_name}\")\n",
        "            label_tokens[label_val] = token_id\n",
        "        except:\n",
        "            try:\n",
        "                token_id = model.to_single_token(label_name)\n",
        "                label_tokens[label_val] = token_id\n",
        "            except:\n",
        "                tokens = model.tokenizer.encode(f\" {label_name}\", add_special_tokens=False)\n",
        "                if tokens:\n",
        "                    label_tokens[label_val] = tokens[0]\n",
        "    return label_tokens\n",
        "\n",
        "\n",
        "def evaluate_candidates_batched(model, base_tokens, position, candidate_tokens, correct_label_token, batch_size=32):\n",
        "    \"\"\"Evaluate multiple candidate tokens in a batched forward pass.\"\"\"\n",
        "    # Filter out token 0 and invalid candidates\n",
        "    valid_candidates = [t for t in candidate_tokens if t != 0]\n",
        "    if not valid_candidates:\n",
        "        return []\n",
        "    \n",
        "    objectives = []\n",
        "    device = base_tokens.device\n",
        "    dtype = base_tokens.dtype\n",
        "    \n",
        "    for i in range(0, len(valid_candidates), batch_size):\n",
        "        batch_candidates = valid_candidates[i:i+batch_size]\n",
        "        \n",
        "        batch_sequences = []\n",
        "        for token_id in batch_candidates:\n",
        "            modified = torch.cat([\n",
        "                base_tokens[:position],\n",
        "                torch.tensor([token_id], device=device, dtype=dtype),\n",
        "                base_tokens[position:]\n",
        "            ])\n",
        "            batch_sequences.append(modified)\n",
        "        \n",
        "        batch_tensor = torch.stack(batch_sequences)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model(batch_tensor)\n",
        "            next_token_logits = logits[:, -1, :]  # [batch, vocab]\n",
        "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            \n",
        "            # compute objective\n",
        "            adv_probs = next_token_probs[:, correct_label_token]\n",
        "            batch_objectives = -torch.log(adv_probs + 1e-10)\n",
        "            objectives.extend(batch_objectives.cpu().tolist())\n",
        "    \n",
        "    return objectives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check A: Objective Monotonicity\n",
        "# extract R2 and R3 values\n",
        "r2_values = []\n",
        "r3_values = []\n",
        "\n",
        "for entry in metadata_list:\n",
        "    rel_improvements = entry.get('relative_improvements', {})\n",
        "    r2 = rel_improvements.get('r2')\n",
        "    r3 = rel_improvements.get('r3')\n",
        "    \n",
        "    if r2 is not None:\n",
        "        r2_values.append(r2)\n",
        "    if r3 is not None:\n",
        "        r3_values.append(r3)\n",
        "\n",
        "# compute statistics\n",
        "if r2_values:\n",
        "    median_r2 = np.median(r2_values)\n",
        "    pct_r2_above_threshold = 100 * sum(1 for r in r2_values if r >= 0.05) / len(r2_values)\n",
        "else:\n",
        "    median_r2 = None\n",
        "    pct_r2_above_threshold = 0.0\n",
        "\n",
        "if r3_values:\n",
        "    median_r3 = np.median(r3_values)\n",
        "    pct_r3_above_threshold = 100 * sum(1 for r in r3_values if r >= 0.05) / len(r3_values)\n",
        "else:\n",
        "    median_r3 = None\n",
        "    pct_r3_above_threshold = 0.0\n",
        "\n",
        "# check pass criteria: median R ≥ 0.05 OR ≥75% examples with R ≥ 0.05\n",
        "r2_passes = (median_r2 is not None and median_r2 >= 0.05) or (pct_r2_above_threshold >= 75.0)\n",
        "r3_passes = (median_r3 is not None and median_r3 >= 0.05) or (pct_r3_above_threshold >= 75.0)\n",
        "\n",
        "check_a_passes = r2_passes and r3_passes\n",
        "\n",
        "# generate histograms\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "if r2_values:\n",
        "    axes[0].hist(r2_values, bins=30, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(0.05, color='red', linestyle='--', label='Threshold (0.05)')\n",
        "    if median_r2 is not None:\n",
        "        axes[0].axvline(median_r2, color='green', linestyle='--', label=f'Median ({median_r2:.3f})')\n",
        "    axes[0].set_xlabel('R2 (Relative Improvement k1→k2)')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Distribution of R2 Values')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[0].text(0.5, 0.5, 'No R2 values', ha='center', va='center')\n",
        "    axes[0].set_title('Distribution of R2 Values')\n",
        "\n",
        "if r3_values:\n",
        "    axes[1].hist(r3_values, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "    axes[1].axvline(0.05, color='red', linestyle='--', label='Threshold (0.05)')\n",
        "    if median_r3 is not None:\n",
        "        axes[1].axvline(median_r3, color='green', linestyle='--', label=f'Median ({median_r3:.3f})')\n",
        "    axes[1].set_xlabel('R3 (Relative Improvement k2→k3)')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title('Distribution of R3 Values')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'No R3 values', ha='center', va='center')\n",
        "    axes[1].set_title('Distribution of R3 Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(ARTIFACTS_DIR, 'sanity_check_a_histograms.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Store results\n",
        "check_a_result = {\n",
        "    \"status\": \"PASS\" if check_a_passes else \"FAIL\",\n",
        "    \"median_r2\": float(median_r2) if median_r2 is not None else None,\n",
        "    \"median_r3\": float(median_r3) if median_r3 is not None else None,\n",
        "    \"pct_r2_above_threshold\": float(pct_r2_above_threshold),\n",
        "    \"pct_r3_above_threshold\": float(pct_r3_above_threshold),\n",
        "    \"r2_passes\": r2_passes,\n",
        "    \"r3_passes\": r3_passes,\n",
        "    \"pass_criteria_met\": check_a_passes\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check B: Tokenization Sanity\n",
        "pathological_cases = []\n",
        "normal_cases = []\n",
        "\n",
        "for entry in metadata_list:\n",
        "    entry_id = entry.get('id', 'unknown')\n",
        "    adv_tokens = entry.get('adv_tokens', [])\n",
        "    adv_decoded = entry.get('adv_decoded', '')\n",
        "    \n",
        "    if not adv_tokens:\n",
        "        continue\n",
        "    \n",
        "    # Decode tokens using model\n",
        "    decoded_tokens = []\n",
        "    issues = []\n",
        "    \n",
        "    for token_id in adv_tokens:\n",
        "        try:\n",
        "            decoded = model.to_string(torch.tensor([token_id]))\n",
        "            decoded_tokens.append(decoded)\n",
        "            \n",
        "            # check for pathological patterns\n",
        "            if len(decoded.strip()) > 0:\n",
        "                if decoded.strip() and not any(c.isalnum() for c in decoded.strip()):\n",
        "                    issues.append(f\"Punctuation-only: '{decoded}'\")\n",
        "                elif decoded.startswith('<|') and decoded.endswith('|>'):\n",
        "                    issues.append(f\"Special token: '{decoded}'\")\n",
        "                elif len(decoded.strip()) < 2 and decoded.strip() and not decoded.strip().isalnum():\n",
        "                    issues.append(f\"Very short fragment: '{decoded}'\")\n",
        "        except Exception as e:\n",
        "            issues.append(f\"Decode error: {e}\")\n",
        "            decoded_tokens.append(f\"<ERROR:{token_id}>\")\n",
        "    \n",
        "    # Compare with adv_decoded field\n",
        "    decoded_str = \" \".join(decoded_tokens)\n",
        "    if decoded_str.strip() != adv_decoded.strip():\n",
        "        issues.append(f\"Mismatch with adv_decoded: decoded='{decoded_str}' vs stored='{adv_decoded}'\")\n",
        "    \n",
        "    if issues:\n",
        "        pathological_cases.append({\n",
        "            \"id\": entry_id,\n",
        "            \"tokens\": adv_tokens,\n",
        "            \"decoded\": decoded_str,\n",
        "            \"stored_decoded\": adv_decoded,\n",
        "            \"issues\": issues\n",
        "        })\n",
        "    else:\n",
        "        normal_cases.append({\n",
        "            \"id\": entry_id,\n",
        "            \"tokens\": adv_tokens,\n",
        "            \"decoded\": decoded_str\n",
        "        })\n",
        "\n",
        "total_cases = len(pathological_cases) + len(normal_cases)\n",
        "pathological_percentage = 100 * len(pathological_cases) / total_cases if total_cases > 0 else 0.0\n",
        "\n",
        "# check pass criteria: <10% pathological cases\n",
        "check_b_passes = pathological_percentage < 10.0\n",
        "\n",
        "# save examples to file\n",
        "examples_file = os.path.join(ARTIFACTS_DIR, 'sanity_check_b_examples.txt')\n",
        "with open(examples_file, 'w') as f:\n",
        "    f.write(\"Tokenization Sanity Check - Pathological Cases\\n\")\n",
        "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "    f.write(f\"Total pathological cases: {len(pathological_cases)} / {total_cases} ({pathological_percentage:.2f}%)\\n\\n\")\n",
        "    \n",
        "    for case in pathological_cases:\n",
        "        f.write(f\"ID: {case['id']}\\n\")\n",
        "        f.write(f\"Tokens: {case['tokens']}\\n\")\n",
        "        f.write(f\"Decoded: '{case['decoded']}'\\n\")\n",
        "        f.write(f\"Stored: '{case['stored_decoded']}'\\n\")\n",
        "        f.write(f\"Issues: {case['issues']}\\n\")\n",
        "        f.write(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "# store results\n",
        "check_b_result = {\n",
        "    \"status\": \"PASS\" if check_b_passes else \"FAIL\",\n",
        "    \"total_prompts\": total_cases,\n",
        "    \"pathological_cases\": len(pathological_cases),\n",
        "    \"pathological_percentage\": float(pathological_percentage),\n",
        "    \"pass_criteria_met\": check_b_passes,\n",
        "    \"examples_count\": len(pathological_cases)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check C: Greedy vs Beam/Exhaustive Search\n",
        "# Compare greedy 3-token results with beam search and mini-exhaustive for 1-2 tokens\n",
        "# Pass if greedy 3-token is superior or equal in ≥90% samples\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Check C: Greedy vs Beam/Exhaustive Search\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Helper function for beam search\n",
        "def beam_search_adv(model, prompt, label, candidate_tokens, positions, label_tokens, \n",
        "                    beam_width=5, max_tokens=2, batch_size=32):\n",
        "    \"\"\"Beam search for adversarial token insertion (up to max_tokens).\"\"\"\n",
        "    tokens = model.to_tokens(prompt)[0]\n",
        "    correct_label_token = label_tokens[label]\n",
        "    \n",
        "    # Initialize beam with empty sequence\n",
        "    beam = [{\n",
        "        'tokens': tokens,\n",
        "        'inserted': [],\n",
        "        'positions': [],\n",
        "        'objective': None\n",
        "    }]\n",
        "    \n",
        "    # Evaluate initial objective\n",
        "    with torch.no_grad():\n",
        "        logits = model(tokens.unsqueeze(0))\n",
        "        next_token_logits = logits[0, -1, :]\n",
        "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "        baseline_obj = -np.log(next_token_probs[correct_label_token].item() + 1e-10)\n",
        "        beam[0]['objective'] = baseline_obj\n",
        "    \n",
        "    # Search for up to max_tokens\n",
        "    for token_num in range(max_tokens):\n",
        "        candidates = []\n",
        "        \n",
        "        for beam_item in beam:\n",
        "            base_tokens = beam_item['tokens']\n",
        "            inserted = beam_item['inserted']\n",
        "            inserted_positions = beam_item['positions']\n",
        "            \n",
        "            # Try inserting at each position\n",
        "            for pos in positions:\n",
        "                if pos >= base_tokens.shape[0]:\n",
        "                    continue\n",
        "                \n",
        "                # Evaluate all candidate tokens at this position\n",
        "                objectives = evaluate_candidates_batched(\n",
        "                    model, base_tokens, pos, candidate_tokens, correct_label_token, batch_size\n",
        "                )\n",
        "                valid_candidates = [t for t in candidate_tokens if t != 0]\n",
        "                \n",
        "                for idx, objective in enumerate(objectives):\n",
        "                    new_token = valid_candidates[idx]\n",
        "                    new_tokens = torch.cat([\n",
        "                        base_tokens[:pos],\n",
        "                        torch.tensor([new_token], device=base_tokens.device, dtype=base_tokens.dtype),\n",
        "                        base_tokens[pos:]\n",
        "                    ])\n",
        "                    \n",
        "                    candidates.append({\n",
        "                        'tokens': new_tokens,\n",
        "                        'inserted': inserted + [new_token],\n",
        "                        'positions': inserted_positions + [pos],\n",
        "                        'objective': objective\n",
        "                    })\n",
        "        \n",
        "        # Keep top beam_width candidates\n",
        "        candidates.sort(key=lambda x: x['objective'], reverse=True)\n",
        "        beam = candidates[:beam_width]\n",
        "        \n",
        "        # Re-evaluate to ensure accuracy\n",
        "        for beam_item in beam:\n",
        "            with torch.no_grad():\n",
        "                logits = model(beam_item['tokens'].unsqueeze(0))\n",
        "                next_token_logits = logits[0, -1, :]\n",
        "                next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "                beam_item['objective'] = -np.log(next_token_probs[correct_label_token].item() + 1e-10)\n",
        "    \n",
        "    # Return best from beam\n",
        "    if beam:\n",
        "        best = max(beam, key=lambda x: x['objective'])\n",
        "        return best['objective'], best['inserted'], best['positions']\n",
        "    return baseline_obj, [], []\n",
        "\n",
        "\n",
        "# Helper function for mini-exhaustive search\n",
        "def mini_exhaustive_search(model, prompt, label, candidate_tokens, positions, label_tokens,\n",
        "                          max_candidates=30, max_tokens=2, batch_size=32):\n",
        "    \"\"\"Mini-exhaustive search: try all combinations of top M candidates for 1-2 tokens.\"\"\"\n",
        "    tokens = model.to_tokens(prompt)[0]\n",
        "    correct_label_token = label_tokens[label]\n",
        "    \n",
        "    # Limit candidate tokens to top M\n",
        "    limited_candidates = candidate_tokens[:max_candidates]\n",
        "    \n",
        "    best_objective = None\n",
        "    best_tokens = []\n",
        "    best_positions = []\n",
        "    \n",
        "    # 1-token search\n",
        "    for pos in positions:\n",
        "        if pos >= tokens.shape[0]:\n",
        "            continue\n",
        "        objectives = evaluate_candidates_batched(\n",
        "            model, tokens, pos, limited_candidates, correct_label_token, batch_size\n",
        "        )\n",
        "        valid_candidates = [t for t in limited_candidates if t != 0]\n",
        "        \n",
        "        for idx, objective in enumerate(objectives):\n",
        "            if best_objective is None or objective > best_objective:\n",
        "                best_objective = objective\n",
        "                best_tokens = [valid_candidates[idx]]\n",
        "                best_positions = [pos]\n",
        "    \n",
        "    # 2-token search (if max_tokens >= 2)\n",
        "    if max_tokens >= 2 and best_tokens:\n",
        "        tokens_1 = torch.cat([\n",
        "            tokens[:best_positions[0]],\n",
        "            torch.tensor([best_tokens[0]], device=tokens.device, dtype=tokens.dtype),\n",
        "            tokens[best_positions[0]:]\n",
        "        ])\n",
        "        \n",
        "        for pos2 in positions:\n",
        "            if pos2 >= tokens_1.shape[0]:\n",
        "                continue\n",
        "            objectives = evaluate_candidates_batched(\n",
        "                model, tokens_1, pos2, limited_candidates, correct_label_token, batch_size\n",
        "            )\n",
        "            valid_candidates = [t for t in limited_candidates if t != 0]\n",
        "            \n",
        "            for idx, objective in enumerate(objectives):\n",
        "                if objective > best_objective:\n",
        "                    best_objective = objective\n",
        "                    best_tokens = [best_tokens[0], valid_candidates[idx]]\n",
        "                    best_positions = [best_positions[0], pos2]\n",
        "    \n",
        "    # Re-evaluate final best\n",
        "    if best_tokens:\n",
        "        final_tokens = tokens.clone()\n",
        "        for tok, pos in zip(best_tokens, sorted(best_positions)):\n",
        "            offset = sum(1 for p in best_positions if p < pos)\n",
        "            insert_pos = pos + offset\n",
        "            final_tokens = torch.cat([\n",
        "                final_tokens[:insert_pos],\n",
        "                torch.tensor([tok], device=final_tokens.device, dtype=final_tokens.dtype),\n",
        "                final_tokens[insert_pos:]\n",
        "            ])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = model(final_tokens.unsqueeze(0))\n",
        "            next_token_logits = logits[0, -1, :]\n",
        "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "            best_objective = -np.log(next_token_probs[correct_label_token].item() + 1e-10)\n",
        "    \n",
        "    return best_objective, best_tokens, best_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Check C: Compare greedy 3-token with beam/exhaustive\n",
        "N_SAMPLES = 20\n",
        "np.random.seed(SEED)\n",
        "sample_indices = np.random.choice(len(metadata_list), size=min(N_SAMPLES, len(metadata_list)), replace=False)\n",
        "\n",
        "label_tokens = get_label_token_ids(model)\n",
        "candidate_tokens = build_candidate_token_list(model, n_tokens=500)\n",
        "\n",
        "comparison_results = []\n",
        "greedy_superior_count = 0\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    entry = metadata_list[idx]\n",
        "    prompt = entry['clean_prompt']\n",
        "    label = entry['clean_label']\n",
        "    \n",
        "    # Get greedy 3-token objective from metadata\n",
        "    greedy_obj = entry['objective_k'].get('k3')\n",
        "    if greedy_obj is None:\n",
        "        continue\n",
        "    \n",
        "    # determine positions\n",
        "    tokens = model.to_tokens(prompt)[0]\n",
        "    review_text = prompt.split(\"\\nSentiment:\")[0]\n",
        "    review_tokens = model.to_tokens(review_text)[0]\n",
        "    max_insertion_pos = review_tokens.shape[0] - 1\n",
        "    \n",
        "    positions = []\n",
        "    if max_insertion_pos > 1:\n",
        "        positions.append(1)\n",
        "    if max_insertion_pos > 3:\n",
        "        positions.append(min(max_insertion_pos // 2, max_insertion_pos - 1))\n",
        "    if max_insertion_pos > 2:\n",
        "        positions.append(max_insertion_pos)\n",
        "    positions = sorted(set([p for p in positions if 0 < p <= max_insertion_pos]))[:3]\n",
        "    \n",
        "    # Run beam search (1-2 tokens)\n",
        "    beam_obj, beam_tokens, beam_positions = beam_search_adv(\n",
        "        model, prompt, label, candidate_tokens, positions, label_tokens,\n",
        "        beam_width=5, max_tokens=2, batch_size=32\n",
        "    )\n",
        "    \n",
        "    # Run mini-exhaustive (1-2 tokens)\n",
        "    exhaustive_obj, exhaustive_tokens, exhaustive_positions = mini_exhaustive_search(\n",
        "        model, prompt, label, candidate_tokens, positions, label_tokens,\n",
        "        max_candidates=30, max_tokens=2, batch_size=32\n",
        "    )\n",
        "    \n",
        "    # Compare: greedy 3-token should be ≥ best alternative\n",
        "    best_alternative = max(beam_obj, exhaustive_obj) if (beam_obj is not None and exhaustive_obj is not None) else None\n",
        "    \n",
        "    if best_alternative is not None:\n",
        "        greedy_superior = greedy_obj >= best_alternative\n",
        "        if greedy_superior:\n",
        "            greedy_superior_count += 1\n",
        "        \n",
        "        comparison_results.append({\n",
        "            'prompt_id': entry['id'],\n",
        "            'greedy_obj': greedy_obj,\n",
        "            'beam_obj': beam_obj,\n",
        "            'exhaustive_obj': exhaustive_obj,\n",
        "            'best_alternative': best_alternative,\n",
        "            'greedy_superior': greedy_superior\n",
        "        })\n",
        "\n",
        "greedy_superior_percentage = 100 * greedy_superior_count / len(comparison_results) if comparison_results else 0.0\n",
        "\n",
        "# check pass criteria: ≥90% samples where greedy 3-token ≥ alternatives\n",
        "check_c_passes = greedy_superior_percentage >= 90.0\n",
        "\n",
        "# save comparison results\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "comparison_file = os.path.join(ARTIFACTS_DIR, 'sanity_check_c_comparison.csv')\n",
        "comparison_df.to_csv(comparison_file, index=False)\n",
        "\n",
        "# generate plot\n",
        "if comparison_results:\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    prompt_ids = [r['prompt_id'] for r in comparison_results]\n",
        "    greedy_objs = [r['greedy_obj'] for r in comparison_results]\n",
        "    beam_objs = [r['beam_obj'] for r in comparison_results]\n",
        "    exhaustive_objs = [r['exhaustive_obj'] for r in comparison_results]\n",
        "    \n",
        "    x = np.arange(len(prompt_ids))\n",
        "    width = 0.25\n",
        "    \n",
        "    ax.bar(x - width, greedy_objs, width, label='Greedy 3-token', color='green', alpha=0.7)\n",
        "    ax.bar(x, beam_objs, width, label='Beam Search (1-2 tokens)', color='blue', alpha=0.7)\n",
        "    ax.bar(x + width, exhaustive_objs, width, label='Mini-Exhaustive (1-2 tokens)', color='orange', alpha=0.7)\n",
        "    \n",
        "    ax.set_xlabel('Prompt ID')\n",
        "    ax.set_ylabel('Objective (negative log-prob)')\n",
        "    ax.set_title('Greedy 3-token vs Beam/Exhaustive Search Comparison')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(prompt_ids, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(ARTIFACTS_DIR, 'sanity_check_c_comparison.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# store results\n",
        "check_c_result = {\n",
        "    \"status\": \"PASS\" if check_c_passes else \"FAIL\",\n",
        "    \"n_samples\": len(comparison_results),\n",
        "    \"greedy_superior_count\": greedy_superior_count,\n",
        "    \"greedy_superior_percentage\": float(greedy_superior_percentage),\n",
        "    \"pass_criteria_met\": check_c_passes\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check D: Marginal Ablation\n",
        "# filter to 3-token sequences only\n",
        "three_token_entries = [entry for entry in metadata_list if entry.get('chosen_k') == 3]\n",
        "\n",
        "# extract ablation marginals\n",
        "marginals_token1 = []\n",
        "marginals_token2 = []\n",
        "marginals_token3 = []\n",
        "\n",
        "for entry in three_token_entries:\n",
        "    ablation_marginals = entry.get('ablation_marginals', [])\n",
        "    \n",
        "    if len(ablation_marginals) >= 3:\n",
        "        marginals_token1.append(ablation_marginals[0])\n",
        "        marginals_token2.append(ablation_marginals[1])\n",
        "        marginals_token3.append(ablation_marginals[2])\n",
        "    elif len(ablation_marginals) == 2:\n",
        "        # If only 2 values, assume they're for token2 and token3\n",
        "        marginals_token2.append(ablation_marginals[0])\n",
        "        marginals_token3.append(ablation_marginals[1])\n",
        "    elif len(ablation_marginals) == 1:\n",
        "        marginals_token3.append(ablation_marginals[0])\n",
        "\n",
        "# compute averages\n",
        "avg_marginal_token1 = np.mean(marginals_token1) if marginals_token1 else None\n",
        "avg_marginal_token2 = np.mean(marginals_token2) if marginals_token2 else None\n",
        "avg_marginal_token3 = np.mean(marginals_token3) if marginals_token3 else None\n",
        "\n",
        "# check pass criteria: |avg marginal fraction of token3| ≥ 0.10\n",
        "avg_marginal_token3_abs = abs(avg_marginal_token3) if avg_marginal_token3 is not None else None\n",
        "check_d_passes = avg_marginal_token3_abs is not None and avg_marginal_token3_abs >= 0.10\n",
        "\n",
        "# generate plot\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "if marginals_token1 or marginals_token2 or marginals_token3:\n",
        "    data_to_plot = []\n",
        "    labels = []\n",
        "    \n",
        "    if marginals_token1:\n",
        "        data_to_plot.append(marginals_token1)\n",
        "        labels.append('Token 1')\n",
        "    if marginals_token2:\n",
        "        data_to_plot.append(marginals_token2)\n",
        "        labels.append('Token 2')\n",
        "    if marginals_token3:\n",
        "        data_to_plot.append(marginals_token3)\n",
        "        labels.append('Token 3')\n",
        "    \n",
        "    if data_to_plot:\n",
        "        bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
        "        \n",
        "        # Color the boxes\n",
        "        colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
        "            patch.set_facecolor(color)\n",
        "        \n",
        "        ax.axhline(0.10, color='red', linestyle='--', label='Threshold (0.10)')\n",
        "        ax.axhline(-0.10, color='red', linestyle='--', alpha=0.5, label='Threshold (-0.10)')\n",
        "        if avg_marginal_token3 is not None:\n",
        "            ax.axhline(avg_marginal_token3, color='green', linestyle='--', \n",
        "                      label=f'Token 3 Mean ({avg_marginal_token3:.3f})')\n",
        "            ax.axhline(-avg_marginal_token3, color='green', linestyle=':', alpha=0.5,\n",
        "                      label=f'|Token 3 Mean| ({abs(avg_marginal_token3):.3f})')\n",
        "        \n",
        "        ax.set_ylabel('Marginal Contribution')\n",
        "        ax.set_title('Marginal Ablation Contributions by Token Position')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'No marginal data available', ha='center', va='center')\n",
        "    ax.set_title('Marginal Ablation Contributions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(ARTIFACTS_DIR, 'sanity_check_d_marginals.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# store results\n",
        "check_d_result = {\n",
        "    \"status\": \"PASS\" if check_d_passes else \"FAIL\",\n",
        "    \"n_3token_sequences\": len(three_token_entries),\n",
        "    \"avg_marginal_token1\": float(avg_marginal_token1) if avg_marginal_token1 is not None else None,\n",
        "    \"avg_marginal_token2\": float(avg_marginal_token2) if avg_marginal_token2 is not None else None,\n",
        "    \"avg_marginal_token3\": float(avg_marginal_token3) if avg_marginal_token3 is not None else None,\n",
        "    \"avg_marginal_token3_abs\": float(avg_marginal_token3_abs) if avg_marginal_token3_abs is not None else None,\n",
        "    \"pass_criteria_met\": check_d_passes,\n",
        "    \"note\": \"Using absolute value: negative marginals indicate tokens are helpful\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check E: Patch Sanity Pilot (N=30)\n",
        "N_SAMPLES_E = 30\n",
        "np.random.seed(SEED)\n",
        "sample_indices_e = np.random.choice(len(metadata_list), size=min(N_SAMPLES_E, len(metadata_list)), replace=False)\n",
        "\n",
        "# generate 1-token adversarial prompts\n",
        "label_tokens_e = get_label_token_ids(model)\n",
        "candidate_tokens_e = build_candidate_token_list(model, n_tokens=500)\n",
        "\n",
        "one_token_prompts = []\n",
        "one_token_objectives = []\n",
        "three_token_objectives = []\n",
        "\n",
        "for i, idx in enumerate(sample_indices_e):\n",
        "    entry = metadata_list[idx]\n",
        "    prompt = entry['clean_prompt']\n",
        "    label = entry['clean_label']\n",
        "    \n",
        "    # Get 3-token objective from metadata\n",
        "    three_token_obj = entry['objective_k'].get('k3')\n",
        "    if three_token_obj is None:\n",
        "        continue\n",
        "    \n",
        "    # Generate 1-token adversarial prompt\n",
        "    tokens = model.to_tokens(prompt)[0]\n",
        "    review_text = prompt.split(\"\\nSentiment:\")[0]\n",
        "    review_tokens = model.to_tokens(review_text)[0]\n",
        "    max_insertion_pos = review_tokens.shape[0] - 1\n",
        "    \n",
        "    positions = []\n",
        "    if max_insertion_pos > 1:\n",
        "        positions.append(1)\n",
        "    if max_insertion_pos > 3:\n",
        "        positions.append(min(max_insertion_pos // 2, max_insertion_pos - 1))\n",
        "    if max_insertion_pos > 2:\n",
        "        positions.append(max_insertion_pos)\n",
        "    positions = sorted(set([p for p in positions if 0 < p <= max_insertion_pos]))[:3]\n",
        "    \n",
        "    # Find best 1-token insertion\n",
        "    best_obj = None\n",
        "    best_token = None\n",
        "    best_pos = None\n",
        "    \n",
        "    correct_label_token = label_tokens_e[label]\n",
        "    \n",
        "    for pos in positions:\n",
        "        if pos >= tokens.shape[0]:\n",
        "            continue\n",
        "        objectives = evaluate_candidates_batched(\n",
        "            model, tokens, pos, candidate_tokens_e, correct_label_token, batch_size=32\n",
        "        )\n",
        "        valid_candidates = [t for t in candidate_tokens_e if t != 0]\n",
        "        \n",
        "        for idx_obj, objective in enumerate(objectives):\n",
        "            if best_obj is None or objective > best_obj:\n",
        "                best_obj = objective\n",
        "                best_token = valid_candidates[idx_obj]\n",
        "                best_pos = pos\n",
        "    \n",
        "    # Re-evaluate to ensure accuracy\n",
        "    if best_token is not None and best_pos is not None:\n",
        "        tokens_1 = torch.cat([\n",
        "            tokens[:best_pos],\n",
        "            torch.tensor([best_token], device=tokens.device, dtype=tokens.dtype),\n",
        "            tokens[best_pos:]\n",
        "        ])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits_1 = model(tokens_1.unsqueeze(0))\n",
        "            next_token_logits_1 = logits_1[0, -1, :]\n",
        "            next_token_probs_1 = torch.softmax(next_token_logits_1, dim=-1)\n",
        "            best_obj = -np.log(next_token_probs_1[correct_label_token].item() + 1e-10)\n",
        "        \n",
        "        one_token_prompts.append(model.to_string(tokens_1))\n",
        "        one_token_objectives.append(best_obj)\n",
        "        three_token_objectives.append(three_token_obj)\n",
        "\n",
        "if len(one_token_objectives) > 0 and len(three_token_objectives) > 0:\n",
        "    avg_obj_1token = np.mean(one_token_objectives)\n",
        "    avg_obj_3token = np.mean(three_token_objectives)\n",
        "    \n",
        "    # check if 3-token shows equal-or-stronger performance\n",
        "    check_e_passes = avg_obj_3token >= avg_obj_1token\n",
        "    \n",
        "    check_e_result = {\n",
        "        \"status\": \"PASS\" if check_e_passes else \"FAIL\",\n",
        "        \"n_samples\": len(one_token_prompts),\n",
        "        \"avg_objective_1token\": float(avg_obj_1token),\n",
        "        \"avg_objective_3token\": float(avg_obj_3token),\n",
        "        \"pass_criteria_met\": bool(check_e_passes)\n",
        "    }\n",
        "else:\n",
        "    check_e_result = {\n",
        "        \"status\": \"ERROR\",\n",
        "        \"reason\": \"Insufficient 1-token prompts generated\",\n",
        "        \"pass_criteria_met\": None\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Final Sanity Report\n",
        "# helper function to convert numpy types to native Python types for JSON serialization\n",
        "def convert_to_json_serializable(obj):\n",
        "    \"\"\"Recursively convert numpy types and other non-serializable types to Python native types.\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_json_serializable(item) for item in obj]\n",
        "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, (np.bool_, bool)):\n",
        "        return bool(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif pd.isna(obj):\n",
        "        return None\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# aggregate all check results\n",
        "all_checks = {\n",
        "    \"check_a\": check_a_result,\n",
        "    \"check_b\": check_b_result,\n",
        "    \"check_c\": check_c_result if 'check_c_result' in globals() else {\"status\": \"NOT_RUN\"},\n",
        "    \"check_d\": check_d_result,\n",
        "    \"check_e\": check_e_result if 'check_e_result' in globals() else {\"status\": \"NOT_RUN\"}\n",
        "}\n",
        "\n",
        "# Determine overall status\n",
        "check_statuses = []\n",
        "for check_name, check_data in all_checks.items():\n",
        "    status = check_data.get('status', 'UNKNOWN')\n",
        "    if status in ['PASS', 'FAIL']:\n",
        "        check_statuses.append(status)\n",
        "\n",
        "overall_passes = sum(1 for s in check_statuses if s == 'PASS')\n",
        "overall_fails = sum(1 for s in check_statuses if s == 'FAIL')\n",
        "overall_status = \"PASS\" if overall_fails == 0 and overall_passes > 0 else \"FAIL\"\n",
        "\n",
        "# generate recommendation\n",
        "if overall_status == \"PASS\":\n",
        "    recommendation = \"Proceed\"\n",
        "else:\n",
        "    recommendation = \"Adjust search / Apply stopping rule\"\n",
        "\n",
        "# create report\n",
        "sanity_report = {\n",
        "    \"check_a\": all_checks[\"check_a\"],\n",
        "    \"check_b\": all_checks[\"check_b\"],\n",
        "    \"check_c\": all_checks[\"check_c\"],\n",
        "    \"check_d\": all_checks[\"check_d\"],\n",
        "    \"check_e\": all_checks[\"check_e\"],\n",
        "    \"overall_status\": overall_status,\n",
        "    \"recommendation\": recommendation,\n",
        "    \"timestamp\": datetime.now().isoformat(),\n",
        "    \"summary\": {\n",
        "        \"total_checks\": len([c for c in all_checks.values() if c.get('status') in ['PASS', 'FAIL']]),\n",
        "        \"passed\": overall_passes,\n",
        "        \"failed\": overall_fails,\n",
        "        \"skipped\": len([c for c in all_checks.values() if c.get('status') not in ['PASS', 'FAIL']])\n",
        "    }\n",
        "}\n",
        "\n",
        "# convert to JSON-serializable format\n",
        "sanity_report = convert_to_json_serializable(sanity_report)\n",
        "\n",
        "# save report\n",
        "report_file = os.path.join(ARTIFACTS_DIR, 'sanity_report.json')\n",
        "with open(report_file, 'w') as f:\n",
        "    json.dump(sanity_report, f, indent=2)\n",
        "\n",
        "# generate summary visualization\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "check_names = ['Check A\\n(Monotonicity)', 'Check B\\n(Tokenization)', \n",
        "               'Check C\\n(Greedy vs Beam)', 'Check D\\n(Marginal)', \n",
        "               'Check E\\n(Patch Sanity)']\n",
        "check_statuses_viz = [\n",
        "    all_checks['check_a']['status'],\n",
        "    all_checks['check_b']['status'],\n",
        "    all_checks['check_c'].get('status', 'NOT_RUN'),\n",
        "    all_checks['check_d']['status'],\n",
        "    all_checks['check_e'].get('status', 'NOT_RUN')\n",
        "]\n",
        "\n",
        "colors = []\n",
        "for status in check_statuses_viz:\n",
        "    if status == 'PASS':\n",
        "        colors.append('green')\n",
        "    elif status == 'FAIL':\n",
        "        colors.append('red')\n",
        "    else:\n",
        "        colors.append('gray')\n",
        "\n",
        "bars = ax.barh(check_names, [1 if s in ['PASS', 'FAIL'] else 0.5 for s in check_statuses_viz], color=colors, alpha=0.7)\n",
        "ax.set_xlim(0, 1.2)\n",
        "ax.set_xlabel('Status')\n",
        "ax.set_title('Sanity Checks Summary Dashboard')\n",
        "ax.set_xticks([0, 0.5, 1])\n",
        "ax.set_xticklabels(['', '', ''])\n",
        "\n",
        "# Add status labels\n",
        "for i, (bar, status) in enumerate(zip(bars, check_statuses_viz)):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.05, bar.get_y() + bar.get_height()/2, status, \n",
        "            ha='left', va='center', fontweight='bold')\n",
        "\n",
        "# Add overall status\n",
        "ax.text(0.5, -0.3, f'Overall: {overall_status} | Recommendation: {recommendation}', \n",
        "        ha='center', va='top', fontsize=12, fontweight='bold',\n",
        "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5),\n",
        "        transform=ax.transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(ARTIFACTS_DIR, 'sanity_check_summary.png'), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
